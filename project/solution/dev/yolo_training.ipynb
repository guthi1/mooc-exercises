{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guthi1/mooc-exercises/blob/daffy-project/project/solution/dev/yolo_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mIJd6b6pbsk"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E2APl4pnbAa"
      },
      "source": [
        "import os\n",
        "import contextlib\n",
        "@contextlib.contextmanager\n",
        "def directory(name):\n",
        "  ret = os.getcwd()\n",
        "  os.chdir(name)\n",
        "  yield None\n",
        "  os.chdir(ret)\n",
        "\n",
        "import subprocess\n",
        "def run(input, exception_on_failure=False):\n",
        "  try:\n",
        "    program_output = subprocess.check_output(f\"{input}\", shell=True, universal_newlines=True, stderr=subprocess.STDOUT)\n",
        "  except Exception as e:\n",
        "    if exception_on_failure:\n",
        "      raise e\n",
        "    program_output = e.output\n",
        "\n",
        "    return program_output\n",
        "\n",
        "def runp(input, exception_on_failure=False):\n",
        "    print(input)\n",
        "    print(run(input, exception_on_failure))\n",
        "\n",
        "#make boxes to xywh format:\n",
        "def xminyminxmaxymax2xywfnormalized(box, image_size):\n",
        "    xmin, ymin, xmax, ymax = np.array(box, dtype=np.float64)\n",
        "    center_x = (xmin+xmax)/2\n",
        "    center_y = (ymin+ymax)/2\n",
        "    width = xmax-xmin\n",
        "    height = ymax-ymin\n",
        "\n",
        "    normalized = np.array([center_x, center_y, width, height])/image_size\n",
        "    return np.round(normalized, 5)\n",
        "\n",
        "def train_test_split(filenames, split_percentage, dataset_dir):\n",
        "    train_txt = np.array(filenames)\n",
        "    np.random.shuffle(train_txt)\n",
        "    nb_things = len(train_txt)\n",
        "    sp = int(split_percentage * nb_things)\n",
        "    train_txt, val_txt = train_txt[:sp], train_txt[sp:]\n",
        "\n",
        "    print(\"ALL IMAGE NAMES TO MOVE DURING THIS SPLIT:\", filenames)\n",
        "    print(\"DATASET DIRECTORY\", dataset_dir)\n",
        "\n",
        "    def mv(img_name, to_train):\n",
        "        print(\"MOVING IMG NAMED\", img_name)\n",
        "\n",
        "        dest = \"train\" if to_train else \"val\"\n",
        "        runp(f\"mv {dataset_dir}/images/{img_name}.jpg {dataset_dir}/{dest}/images/{img_name}.jpg\")\n",
        "        runp(f\"mv {dataset_dir}/labels/{img_name}.txt {dataset_dir}/{dest}/labels/{img_name}.txt\")\n",
        "\n",
        "    for img in train_txt:\n",
        "        mv(img, True)\n",
        "    for img in val_txt:\n",
        "        mv(img, False)\n",
        "\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def makedirs(name):\n",
        "    try:\n",
        "        os.makedirs(name)\n",
        "    except:\n",
        "        pass\n",
        "    yield None\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def directory(name):\n",
        "    ret = os.getcwd()\n",
        "    os.chdir(name)\n",
        "    yield None\n",
        "    os.chdir(ret)\n",
        "\n",
        "def makedirs(name):\n",
        "    try:\n",
        "        os.makedirs(name)\n",
        "    except:\n",
        "        pass\n",
        "    yield None\n",
        "\n",
        "def seed(seed):\n",
        "    # torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "def launch_env(map):\n",
        "    import gym_duckietown\n",
        "    from gym_duckietown.envs import DuckietownEnv\n",
        "    env = DuckietownEnv(\n",
        "        map_name=map,\n",
        "        domain_rand=False,\n",
        "        max_steps=math.inf,\n",
        "    )\n",
        "    return env\n",
        "\n",
        "import cv2\n",
        "\n",
        "def _mod_mask(mask):\n",
        "    temp = mask.copy()\n",
        "    temp[temp == 1] = 50\n",
        "    temp[temp == 2] = 100\n",
        "    temp[temp == 3] = 150\n",
        "    temp[temp == 4] = 200\n",
        "    temp = temp.astype(\"uint8\")\n",
        "    mask = cv2.applyColorMap(temp, cv2.COLORMAP_RAINBOW)\n",
        "    return mask\n",
        "\n",
        "def display_img_seg_mask(real_img, seg_img):\n",
        "    all = np.concatenate(\n",
        "        (cv2.cvtColor(real_img, cv2.COLOR_RGB2BGR), seg_img),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    cv2.imshow(\"image\", all)\n",
        "    cv2.waitKey(0)\n",
        "\n",
        "def prun(input, exception_on_failure=False):\n",
        "  x = run(input, exception_on_failure)\n",
        "  print(x)\n",
        "  return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_DIR=\"/dt_dataset\"\n",
        "IMAGE_SIZE = 416\n",
        "# this is the percentage of data that will go into the training set (as opposed to the testing set)\n",
        "SPLIT_PERCENTAGE = 0.8"
      ],
      "metadata": {
        "id": "H5nzE_-8faaO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## download dataset"
      ],
      "metadata": {
        "id": "wZ3WyoSGgroM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data:\n",
        "runp(f\"rm -rf {DATASET_DIR}\")\n",
        "runp(f\"mkdir {DATASET_DIR}\")\n",
        "runp(f\"mkdir {DATASET_DIR}/images\")\n",
        "runp(f\"mkdir {DATASET_DIR}/labels\")\n",
        "runp(f\"mkdir {DATASET_DIR}/train\")\n",
        "runp(f\"mkdir {DATASET_DIR}/val\")\n",
        "runp(f\"mkdir {DATASET_DIR}/train/images\")\n",
        "runp(f\"mkdir {DATASET_DIR}/train/labels\")\n",
        "runp(f\"mkdir {DATASET_DIR}/val/images\")\n",
        "runp(f\"mkdir {DATASET_DIR}/val/labels\")\n",
        "!wget -O duckietown_object_detection_dataset.zip https://www.dropbox.com/s/bpd535fzmj1pz5w/duckietown%20object%20detection%20dataset-20201129T162330Z-001.zip?dl=0\n",
        "runp(f\"unzip -q duckietown_object_detection_dataset.zip -d {DATASET_DIR}\")\n",
        "runp(f\"mv {DATASET_DIR}/duckietown\\ object\\ detection\\ dataset/* {DATASET_DIR} && rm -rf {DATASET_DIR}/duckietown\\ object\\ detection\\ dataset\")\n",
        "runp(f\"rm duckietown_object_detection_dataset.zip\")"
      ],
      "metadata": {
        "id": "7fMxPPOzfUct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resize the images"
      ],
      "metadata": {
        "id": "vnDDq5Uggaet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "with open(f\"{DATASET_DIR}/annotation/final_anns.json\") as anns:\n",
        "    annotations = json.load(anns)"
      ],
      "metadata": {
        "id": "3DiMVBsVgYye"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "npz_index = 0\n",
        "\n",
        "all_image_names = []\n",
        "    \n",
        "def save_img(img, boxes, classes):\n",
        "    global npz_index\n",
        "    cv2.imwrite(f\"{DATASET_DIR}/images/real_{npz_index}.jpg\", img)\n",
        "    with open(f\"{DATASET_DIR}/labels/real_{npz_index}.txt\", \"w\") as f:\n",
        "        for i in range(len(boxes)):\n",
        "            f.write(f\"{classes[i]} \"+\" \".join(map(str,boxes[i]))+\"\\n\")\n",
        "    npz_index += 1\n",
        "    all_image_names.append(f\"real_{npz_index}\")\n",
        "\n",
        "filenames = tqdm(os.listdir(f\"{DATASET_DIR}/frames\"))\n",
        "for filename in filenames:\n",
        "    img = cv2.imread(f\"{DATASET_DIR}/frames/{filename}\")\n",
        "\n",
        "    orig_y, orig_x = img.shape[0], img.shape[1]\n",
        "    scale_y, scale_x = IMAGE_SIZE/orig_y, IMAGE_SIZE/orig_x\n",
        "\n",
        "    img = cv2.resize(img, (IMAGE_SIZE,IMAGE_SIZE))\n",
        "\n",
        "    boxes = []\n",
        "    classes = []\n",
        "\n",
        "    if filename not in annotations:\n",
        "        continue\n",
        "\n",
        "    for detection in annotations[filename]:\n",
        "        box = detection[\"bbox\"]\n",
        "        label = detection[\"cat_name\"]\n",
        "\n",
        "        if label not in [\"duckie\", \"cone\"]:\n",
        "            continue\n",
        "\n",
        "        orig_x_min, orig_y_min, orig_w, orig_h = box\n",
        "\n",
        "        x_min = int(np.round(orig_x_min * scale_x))\n",
        "        y_min = int(np.round(orig_y_min * scale_y))\n",
        "        x_max = x_min + int(np.round(orig_w * scale_x))\n",
        "        y_max = y_min + int(np.round(orig_h * scale_y))\n",
        "\n",
        "        boxes.append([x_min, y_min, x_max, y_max])\n",
        "        classes.append(1 if label == \"duckie\" else 2)\n",
        "\n",
        "    if len(boxes) == 0:\n",
        "        continue\n",
        "\n",
        "    boxes = np.array([xminyminxmaxymax2xywfnormalized(box, IMAGE_SIZE) for box in boxes])\n",
        "    classes = np.array(classes)-1\n",
        "    \n",
        "    save_img(img, boxes, classes)\n",
        "\n",
        "train_test_split(all_image_names, SPLIT_PERCENTAGE, DATASET_DIR)"
      ],
      "metadata": {
        "id": "0Ui4hBWVgdvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6gJLjcipgNw"
      },
      "source": [
        "Mounts your google drive to move the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQljQye7hdYp",
        "outputId": "dfbcf3b0-c7e1-47a3-e823-85c09fd7cb2f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "runp(f\"mv {DATASET_DIR} /content/drive/MyDrive/dt_dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQQVO21mhkls",
        "outputId": "36660e75-c74c-44ae-b11a-f52369af4ff7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv /dt_dataset /content/drive/MyDrive/dt_dataset\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract dataset from drive"
      ],
      "metadata": {
        "id": "VpivqKqvi7PA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AeEvnIDejCH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runp(f\"cp -r /content/drive/MyDrive/dt_dataset /content{DATASET_DIR} \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKVKA9pqjDfG",
        "outputId": "1b742b29-beee-48ad-e3f7-d72cfe593273"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp -r /content/drive/MyDrive/dt_dataset /content//dt_dataset \n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu1Y8D2jINNm"
      },
      "source": [
        "os.chdir(f'/content{DATASET_DIR}')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0GPE6WDIV0X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad07b102-62fe-4c63-fdf8-33f13035deb8"
      },
      "source": [
        "if not os.path.exists(\"SENTINEL\"):\n",
        "  prun(\"mkdir duckietown_dataset\")\n",
        "  prun(\"mv train duckietown_dataset && mv val duckietown_dataset\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc4VMcwmpr84"
      },
      "source": [
        "## Clone Yolov5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8L68QAeZF9G"
      },
      "source": [
        "!git clone https://github.com/guthi1/yolov5.git -b dt-obj-det\n",
        "!cd yolov5 && pip3 install -r requirements.txt\n",
        "!pip3 install torch==1.11 torchvision==0.12.0\n",
        "if not os.path.exists(\"SENTINEL\"):\n",
        "  run(\"mv duckietown_dataset yolov5\")\n",
        "!touch SENTINEL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CalmQI9Ypx5v"
      },
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logger"
      ],
      "metadata": {
        "id": "hrOG7Ia-rrzU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a loggin system with wandb"
      ],
      "metadata": {
        "id": "FgG4Rbh7obIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q wandb \n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "WLlmSIM2oEd8",
        "outputId": "c6e7b5b6-7a17-4c75-f09e-f51cc8770534"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     || 1.9 MB 4.9 MB/s \n",
            "\u001b[K     || 168 kB 67.2 MB/s \n",
            "\u001b[K     || 182 kB 59.8 MB/s \n",
            "\u001b[K     || 62 kB 1.3 MB/s \n",
            "\u001b[K     || 168 kB 49.5 MB/s \n",
            "\u001b[K     || 166 kB 49.2 MB/s \n",
            "\u001b[K     || 166 kB 72.6 MB/s \n",
            "\u001b[K     || 162 kB 69.9 MB/s \n",
            "\u001b[K     || 162 kB 55.1 MB/s \n",
            "\u001b[K     || 158 kB 70.3 MB/s \n",
            "\u001b[K     || 157 kB 65.2 MB/s \n",
            "\u001b[K     || 157 kB 55.9 MB/s \n",
            "\u001b[K     || 157 kB 53.2 MB/s \n",
            "\u001b[K     || 157 kB 61.3 MB/s \n",
            "\u001b[K     || 157 kB 40.4 MB/s \n",
            "\u001b[K     || 157 kB 80.0 MB/s \n",
            "\u001b[K     || 157 kB 72.6 MB/s \n",
            "\u001b[K     || 156 kB 49.1 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "路路路路路路路路路路\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "ujiwX2FOrt59"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kss7Oid6OkAv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3ea7a4a9-237f-4a39-87f6-0d81d40f451a"
      },
      "source": [
        "!mv yolov5/best.pt yolov5/best_old.pt\n",
        "!cd yolov5 && pip3 install -r requirements.txt && python3 train.py --img 416 --batch 16 --epochs 10 --data duckietown.yaml --weights yolov5s.pt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "all_exps = os.listdir(\"yolov5/runs/train\")\n",
        "all_exps_filtered = map(lambda x: int(x.replace(\"exp\", \"1\")), filter(lambda x: x.startswith(\"exp\"), all_exps))\n",
        "all_exps_filtered = np.array(list(all_exps))\n",
        "latest_exp_index = np.argmax(all_exps)\n",
        "latest_exp = all_exps[latest_exp_index]\n",
        "print(f\"Latest exp is {latest_exp}\")\n",
        "\n",
        "prun(f\"cp yolov5/runs/train/{latest_exp}/weights/best.pt yolov5/best.pt\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'yolov5/best.pt': No such file or directory\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (1.21.6)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (4.6.0.66)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (6.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 9)) (1.7.3)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 10)) (1.11.0)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 11)) (0.12.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 12)) (4.64.1)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 15)) (2.9.1)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 19)) (0.11.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 20)) (1.3.5)\n",
            "Requirement already satisfied: thop in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 28)) (0.1.1.post2209072238)\n",
            "Requirement already satisfied: pycocotools>=2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 29)) (2.0.6)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->-r requirements.txt (line 10)) (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.8.1->-r requirements.txt (line 11)) (2.23.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.3.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (3.4.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (3.19.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.51.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.8.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (0.38.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (2.15.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (0.6.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 20)) (2022.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 15)) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 15)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 15)) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 15)) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 15)) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 15)) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.8.1->-r requirements.txt (line 11)) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.8.1->-r requirements.txt (line 11)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.8.1->-r requirements.txt (line 11)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.8.1->-r requirements.txt (line 11)) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 15)) (3.2.2)\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/duckietown/yolov5 \n",
            "YOLOv5  v5.0-68-ge6681ef torch 1.11.0+cu102 CPU\n",
            "\n",
            "Namespace(adam=False, artifact_alias='latest', batch_size=16, bbox_interval=-1, bucket='', cache_images=False, cfg='', data='./data/duckietown.yaml', device='', entity=None, epochs=10, evolve=False, exist_ok=False, global_rank=-1, hyp='data/hyp.scratch.yaml', image_weights=False, img_size=[416, 416], label_smoothing=0.0, linear_lr=False, local_rank=-1, multi_scale=False, name='exp', noautoanchor=False, nosave=False, notest=False, project='runs/train', quad=False, rect=False, resume=False, save_dir='runs/train/exp3', save_period=-1, single_cls=False, sync_bn=False, total_batch_size=16, upload_dataset=False, weights='yolov5s.pt', workers=8, world_size=1)\n",
            "\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mguthi1\u001b[0m (\u001b[33mxabjuwplb\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/dt_dataset/yolov5/wandb/run-20221207_221811-3d4pd3bg\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mexp3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 猸锔 View project at \u001b[34m\u001b[4mhttps://wandb.ai/xabjuwplb/YOLOv5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/xabjuwplb/YOLOv5/runs/3d4pd3bg\u001b[0m\n",
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  1    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
            "  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     24273  models.yolo.Detect                      [4, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "/usr/local/lib/python3.8/dist-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Model Summary: 283 layers, 7071633 parameters, 7071633 gradients, 16.5 GFLOPS\n",
            "\n",
            "Transferred 356/362 items from yolov5s.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "Optimizer groups: 62 .bias, 62 conv.weight, 59 other\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'duckietown_dataset/train/labels' images and labels... 804 found, 0 missing, 0 empty, 0 corrupted: 100% 804/804 [00:01<00:00, 515.39it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: duckietown_dataset/train/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning 'duckietown_dataset/val/labels' images and labels... 202 found, 0 missing, 0 empty, 0 corrupted: 100% 202/202 [00:00<00:00, 440.93it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: duckietown_dataset/val/labels.cache\n",
            "Plotting labels... \n",
            "Images sizes do not match. This will causes images to be display incorrectly in the UI.\n",
            "\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 4.92, Best Possible Recall (BPR) = 1.0000\n",
            "Image sizes 416 train, 416 test\n",
            "Using 2 dataloader workers\n",
            "Logging results to runs/train/exp3\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "  0% 0/51 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "       0/9        0G    0.1369   0.02011    0.0486    0.2056        64       416:   2% 1/51 [00:16<13:47, 16.55s/it]^C\n",
            "Latest exp is exp3\n",
            "cp: cannot stat 'yolov5/runs/train/exp3/weights/best.pt': No such file or directory\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"cp: cannot stat 'yolov5/runs/train/exp3/weights/best.pt': No such file or directory\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz2PZ7d0qPt0"
      },
      "source": [
        "## Upload model to Duckietown's cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bNXEgAFpRIH"
      },
      "source": [
        "!pip3 install git+https://github.com/duckietown/lib-dt-mooc-2021"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3HWb4wMpZc5"
      },
      "source": [
        "from dt_mooc.cloud import Storage\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "def select_device(device='', batch_size=None):\n",
        "    import torch\n",
        "    # device = 'cpu' or '0' or '0,1,2,3'\n",
        "    cpu = device.lower() == 'cpu'\n",
        "    if cpu:\n",
        "        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # force torch.cuda.is_available() = False\n",
        "    elif device:  # non-cpu device requested\n",
        "        os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable\n",
        "        assert torch.cuda.is_available(), f'CUDA unavailable, invalid device {device} requested'  # check availability\n",
        "\n",
        "    cuda = not cpu and torch.cuda.is_available()\n",
        "\n",
        "    return torch.device('cuda:0' if cuda else 'cpu')\n",
        "\n",
        "sys.path.insert(0, './yolov5')\n",
        "model = torch.load(\"./yolov5/best.pt\", map_location=select_device(\"cpu\"))['model'].float()  # load to FP32\n",
        "model.to(select_device(\"cpu\")).eval()\n",
        "\n",
        "storage = Storage(\"YOUR TOKEN HERE\")\n",
        "\n",
        "storage.upload_yolov5(\"yolov5\", model, \"./yolov5/best.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUVJ5BfBGq7F"
      },
      "source": [
        "# Done!"
      ]
    }
  ]
}